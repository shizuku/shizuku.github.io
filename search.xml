<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>从零开始C++深度学习</title>
      <link href="/2020/03/03/tech/ml/MachineLearningInCpp/"/>
      <url>/2020/03/03/tech/ml/MachineLearningInCpp/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这篇文章原是我上学期的线代课的作业，后来添了一点东西又成了微积分作业，再后来又添了亿点东西又成了计导作业。现在再修修补补又是一篇博客(*￣ ▽ ￣*)ブ。</p><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>本文我们将只使用 STL 来实现一个最简单的手写数字识别（被誉为深度学习的 Hello world），之所以用简单的例子是因为难的我不会(～￣ ▽ ￣)～，或者也可以说是因为简单的例子比较能够看到问题的本质。</p><p>那么进入正题，我们首先列出一个框架，然后逢山开路，遇水造桥。我们知道深度学习涉及大量矩阵运算，因此我们首先需要实现一个矩阵类以表示矩阵及其运算。我们知道 python 中的线性代数库 numpy 的矩阵运算底层就是用 c 实现。一种实现是使用模板的，另一种是不使用模板的。两种实现各有利弊，如果使用模板将更符合数学的直觉——矩阵应该是不能随意改变形状的，并且许多行数和列数的匹配问题（比如矩阵乘法的行列匹配关系）可以在编译时发现；而不用模板的话可以更轻松地实现 reshape 的操作，并且可以更方便隐藏源代码。numpy 显然没有使用模板，而我当初写的时候没有考虑这么多，所以写成模板了(´。＿。｀)，然后就一直凑合用着。</p><p>然后，我们需要读取数据，我们知道深度学习是基于大数据的，没有数据就什么都没有。本文例子所用的数据来自于：<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a></p><p>如果做别的项目，互联网上其实也都有许多数据集可用使用，这就有待于聪明的你去发现了 o(*￣ ▽ ￣*)ブ。</p><p>有了数据集我们还需要读取，然后做一些处理，因此我们需要实现一个 Reader 类来读取数据。</p><p>之后，我们需要构建一个神经网络然后实现预测、训练等功能。</p><p>然后就是 main 用来调度。</p><p>那么本文以下的内容大概以此为主线展开：</p><ul><li>实现 la::matrix</li><li>实现 Reader</li><li>实现 Framework</li><li>实现 main</li></ul><p>我实现的矩阵类放在名称空间 la（linear algebra）中，此外还包含全局的操作符重载和应用于矩阵的函数，而其它几个类并没有塞进名称空间里面，可能稍微有点违和。</p><h2 id="实现-la-matrix"><a href="#实现-la-matrix" class="headerlink" title="实现 la::matrix"></a>实现 la::matrix</h2><p>我们知道 python 中的线性代数库 numpy 的矩阵运算底层就是用 C 实现。对于 C++的实现，我们有两种思路，一种实现是使用模板的，另一种是不使用模板的。两种实现各有利弊，如果使用模板将更符合数学的直觉——矩阵应该是不能随意改变形状的，并且许多行数和列数的匹配问题（比如矩阵乘法的行列匹配关系）可以在编译时发现；而不用模板的话可以更轻松地实现 reshape 的操作，并且可以更方便隐藏源代码。我所采用的是模板类的方式。</p><p>为了方便管理，我们使用一维数组存储数据并提供 at 方法访问。为了应对不同的需求，诸如是否下标检查，是否 const 访问，提供四种 at 方法。为了防止堆栈空间不足而溢出，我们将其动态分配在堆上。<br>此外还可以提供迭代器以供快速迭代。<br>实现基本四则运算以及矩阵乘法，实现基本操作如转置，大多以操作符重载的形式实现。<br>此外还有一些杂乱的功能，例如适用于矩阵的函数，本例用到的其他矩阵操作等。</p><p>以下为部分的代码。</p><pre class="line-numbers language-C++"><code class="language-C++">namespace la {template<size_t _M, size_t _N, typename _Ty = double>class matrix {public:    matrix() {...}    matrix(const std::vector<_Ty>& a) {...}    matrix(const matrix<_M, _N, _Ty>& src) {...}    matrix(const matrix<_M, _N, _Ty>&& src) noexcept{...}    ~matrix() { delete [] head; }    class const_iterator {...};    class iterator {...};    inline void clear() {...}    void fill() {...}    void fill(const std::vector<_Ty>& a) {...}    _Ty& at(size_t m, size_t n) {...}    _Ty& fat(size_t m, size_t n) {...}    const _Ty& cat(size_t m, size_t n) const {...}    const _Ty& fcat(size_t m, size_t n) const {...}    _Ty sum() {...}    size_t max_index() {...}    _Ty maximum(){...}    inline const matrix<_N, _M, _Ty> transposition() const {...}    matrix<_M, _N, _Ty>& operator=(const matrix<_M, _N, _Ty>& src) {...}    const matrix<_M, _N, _Ty> operator+() const {...}    const matrix<_M, _N, _Ty> operator-() const {...}    const matrix<_N, _M, _Ty> operator~() const {...}    matrix<_M, _N, _Ty>& operator+=(const matrix<_M, _N, _Ty> src) {...}        ...    matrix::iterator begin() {...}    matrix::iterator end() {...}        ...private:    _Ty* head;    _Ty* tail;};template<size_t _A, size_t _B, typename _Ty>la::matrix<_A, _B, _Ty> operator+(const la::matrix<_A, _B, _Ty>& a, const la::matrix<_A, _B, _Ty>& b) {...}...template<size_t _A, size_t _B, size_t _C, typename _Ty>la::matrix<_A, _C, _Ty> operator%(const la::matrix<_A, _B, _Ty>& a, const la::matrix<_B, _C, _Ty>& b) {...}template<size_t _A, size_t _B, typename _Ty>std::ostream& operator<<(std::ostream& ostr, const la::matrix<_A, _B, _Ty>& a) {...}template<size_t _A, size_t _B, size_t _C, size_t _D, typename _Ty>const la::matrix<_A, _B, _Ty> reshape(const la::matrix<_C, _D, _Ty>& x) {...}...};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="实现-Reader"><a href="#实现-Reader" class="headerlink" title="实现 Reader"></a>实现 Reader</h2><p>根据(mnist)页面的描述，我们编写一个类来读取此数据集。</p><p>根据描述，(img)数据的开头为四个 32 位(int)，我们将其读取并忽略。而(lab)数据开头为两个 32 位(int)，同样读取并忽略。如构造函数所示。</p><p>img 数据为四位表示一个像素点，我们需要连续读取四个位并将其反转，读取这样的 784 个数字并且将其存储在矩阵中，然后返回。lab 数据只需要读取一个数字并返回即可。</p><p>在(get_img)和(get_lab)函数中用(read)方法读取，(img)数据需要反转，所以添加(private)方法(reverse_32)。</p><p>简略代码如下：</p><pre class="line-numbers language-C++"><code class="language-C++">template<size_t _M, size_t _N>class Reader {public:    Reader(std::string img, std::string lab) {...}    const la::matrix<_M, _N> get_img(const size_t index) {...}    const char get_lab(const int index) {...}private:    unsigned int reverse_32(unsigned int n) {...}    std::ifstream img;    std::ifstream lab;};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="实现-Framework"><a href="#实现-Framework" class="headerlink" title="实现 Framework"></a>实现 Framework</h2><p>接下来进行神经网络的实现，我们的神经网络非常简单，只有一个隐藏层，隐藏层也只有十个神经元。对此我们编写一个类实现。类包含 b0、(b1)、(w1)三个矩阵。此外还实现了预测(predict)和训练(train)的功能。</p><p>在构造函数中我们随机初始化(w1)矩阵，随机区间为([-\sqrt{\frac{6}{28 \times 28 + 10}},\sqrt{\frac{6}{28 \times 28 + 10}}])，(b0)、(b1)则全部为 0。</p><p>预测只需要根据式(5)进行即可。</p><pre class="line-numbers language-C++"><code class="language-C++">la::matrix<1, 10> NeuralNetwork::predict(const la::matrix<1, 28 * 28>& in) {    return f2((((f1((in + b0))) % w1) + b1));}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>训练需要以 train_batch 为单位进行，我们以 100 个数据为一组对神经网络进行调整，总共 600 组。</p><pre class="line-numbers language-C++"><code class="language-C++">void NeuralNetwork::train(Reader<size1, size1>& data) {    la::matrix<1, size1 * size1> b0tmp{};    la::matrix<1, size2> b1tmp{};    la::matrix<size1 * size1, 10> w1tmp{};    int lab{};    la::matrix<1, size1 * size1> in{};    la::matrix<1, size1 * size1> l0_in{};    la::matrix<1, size1 * size1> l0_out{};    la::matrix<1, size2> l1_in{};    la::matrix<1, size2> l1_out{};    la::matrix<size2, 1> act1{};    la::matrix<size2, 1> grad_b1{};    la::matrix<size1 * size1, 10> grad_w1{};    la::matrix<1, size1 * size1> grad_b0{};    for (int i = 0; i < 600; i++) {        for (int j = 0; j < 100; j++) {            lab = data.get_lab(0);            in = la::reshape<1, size1 * size1>(data.get_img(0));            l0_in = in + b0;            l0_out = f1(l0_in);            l1_in = ((l0_out % w1) + b1);            l1_out = f2(l1_in);            act1 = (df2(l1_in) % ~(la::matrix<1, size2>(identity(lab)) - l1_out));            grad_b1 = -2 * act1;            grad_w1 = -2 * la::outer(l0_out, ~act1);            grad_b0 = -2 * (df1(l0_in) * ~(w1 % act1));            b1tmp += ~grad_b1;            w1tmp += grad_w1;            b0tmp += grad_b0;        }        b0 -= (b0tmp / 100) * learn_rate;        w1 -= (w1tmp / 100) * learn_rate;        b1 -= (b1tmp / 100) * learn_rate;        b0tmp.clear();        w1tmp.clear();        b1tmp.clear();        std::cout << i << "\n";    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以下为类的声明：</p><pre class="line-numbers language-C++"><code class="language-C++">const size_t size1 = 28;const size_t size2 = 10;class NeuralNetwork {public:    NeuralNetwork();    la::matrix<1,10> predict(const la::matrix<1, 28 * 28>& in);    void train(Reader<size1, size1>& data);    void train(const std::string& img, const std::string& lab);    void save();    void read();private:    static const std::vector<double> identity(int a, int b = 10);    double learn_rate;    la::matrix<1, size1 * size1> b0;    la::matrix<1, size2> b1;    la::matrix<size1 * size1, 10> w1;};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="实现-main"><a href="#实现-main" class="headerlink" title="实现 main"></a>实现 main</h2><p>main 模块主要包含 main 函数调度和正确率计算功能。</p><p>我们在 main 函数中实例化神经网络，并进行训练，分别输出训练前和训练后的证确率。代码如下：</p><pre class="line-numbers language-C++"><code class="language-C++">int main() {    NeuralNetwork dm = NeuralNetwork();    std::cout << test(dm, test_img_filename, test_lab_filename) << "\n";    system("pause");    dm.train(train_img_filename, train_lab_filename);    std::cout << test(dm, test_img_filename, test_lab_filename) << "\n";    system("pause");}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>正确率计算并不困难，我们只需要对比期望的结果和实际的结果，并计数即可，代码如下：</p><pre class="line-numbers language-C++"><code class="language-C++">double test(NeuralNetwork& dm, const std::string& img, const std::string& lab){    Reader<size1, size1> test(test_img_filename, test_lab_filename);    int a = 0, b = 0;    std::vector<int> num1(10,0);    std::vector<int> num2(10,0);    for (int i = 0; i < test_num; ++i) {        auto img_i = test.get_img(0);        int lab_i = (int)test.get_lab(0);        auto rimg = dm.predict(la::reshape<1, 28 * 28>(img_i));        int m = (int)rimg.max_index();        if (m == lab_i) {            a++; b++;        }        else {            b++;        }        num1[m]++;        num2[lab_i]++;    }    return double(a) / double(b);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如图为实验的结果，虽然由于神经网络的结构过于简单，致使训练后的正确率并不高，但这样的结果也符合我们的预期。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
